[
  {
    "objectID": "Chapter 2.html",
    "href": "Chapter 2.html",
    "title": "2  Xaringan",
    "section": "",
    "text": "2.1 Xaringan Presentation - MODIS\nThe content for week 2 is a presentation in Xaringan on MODIS. The presentation consisting of a summary, application and reflection are all included in the slides below:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Xaringan</span>"
    ]
  },
  {
    "objectID": "Chapter 3.html",
    "href": "Chapter 3.html",
    "title": "3  Remote Sensing Data",
    "section": "",
    "text": "3.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "Chapter 3.html#application",
    "href": "Chapter 3.html#application",
    "title": "3  Remote Sensing Data",
    "section": "3.2 Application",
    "text": "3.2 Application\nIn this week’s practical, the focus was on image enhancements such as band ratioing, filter, texture, data fusion, and Principal Component Analysis (PCA). In band ratioing, the contrast between features are enhanced by “dividing the brightness values (digital numbers) at peaks/maxima and troughs/minima in a reflectance curve after the additive atmospheric haze and additive sensor offset have been removed” (Ghrefat et al. 2023). One common example of band ratioing is the Normalised Difference Vegetation Index (NDVI) used to examine vegetation density and health (USGS, n.d.b). NDVI is calculated by taking the ratio between the red (R) and near infrared (NIR) values and can be represented by the equation:\nNDVI = \\(\\frac{NIR - Red}{NIR+Red}\\)\n\n\n\n\n\n\n\n\n\nFrom left to right: all NDVI values for Cape Town, South Africa; NDVI values above 0.20 for Cape Town, South Africa. Source: Author’s Own.\nNumerous studies have used NDVI to study vegetation which includes the use of NDVI to monitor vegetation dynamics for Africa between 1982 and 1983 (Tucker, Townshend, and Goff 1985). The study examined how phenomenon such as the movement of the Intertropical Convergence Zone or the 1983 drought in the Sahel influenced vegetation density and productivity given these changes in rainfall.\nHowever, a limitation of NDVI is the saturation effect for densely vegetated surfaces such as forested grounds (Jiang et al. 2007). Other indices such as Enhanced Vegetation Index (EVI) are suggested to overcome this drawback and reduce background noise and atmospheric interference. Overall, this means that the signals observed is primarily due to vegetation, making it more accurate to visually assess plant health.\n\n\n\n\n\n\n\n\n\nFrom left to right: Landsat Surface Reflectance; NDVI derived image: EVI derived image. Source: USGS (n.d.b); USGS (n.d.a)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "Chapter 3.html#reflection",
    "href": "Chapter 3.html#reflection",
    "title": "3  Remote Sensing Data",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nThis week’s lecture was content heavy with the different types of factors that can influence the quality of images, methods of corrections, along with image enhancement techniques. Even though many images come pre-processed nowadays, the lecture was still useful in gaining an appreciation for the processes involved before an image is ready for analysis. In the broader scheme, it is important to question and understand the assumptions underlying data processing, as biasness could be introduced.\nFurthermore, there could be cases where the images are not ready for analysis and require some additional processing. This is quite likely given that I sometimes use my drone for photography and I am hoping to explore what kind of analysis I can do with the images in the future.\n\n\n\n\n\n\n\n\n\nPulau Besar in Malaysia taken from Dji Mini 2. Source: Author’s Own.\n\n\n\n\nAl-amri, Salem Saleh, N. V. Kalyankar, and S. D. Khamitkar. 2010. “Contrast Stretching Enhancement in Remote Sensing Image.” International Journal of Computer Science Issues (IJCSI) 7 (2): 26–29.\n\n\nBasith, Abdul. 2011. “Landslide Susceptibility Modelling Under Environmental Changes: A Case Study of Cameron Highlands, Malaysia.” PhD thesis, Perak. https://www.researchgate.net/publication/320710942_LANDSLIDE_SUSCEPTIBILITY_MODELLING_UNDER_ENVIRONMENTAL_CHANGES_A_CASE_STUDY_OF_CAMERON_HIGHLANDS_MALAYSIA.\n\n\nEarth, Penn State College of, and Mineral Science. n.d. “14. Image Correction.” https://www.e-education.psu.edu/natureofgeoinfo/node/1896.\n\n\nGhrefat, Habes, Muheeb Awawdeh, Fares Howari, and Abdulla Al-Rawabdeh. 2023. “Chapter 12 - Mineral Exploration Using Multispectral and Hyperspectral Remote Sensing Data.” In, edited by Nikolaos Stathopoulos, Andreas Tsatsaris, and Kleomenis Kalogeropoulos, 197–222. Earth Observation. Elsevier.\n\n\nGovernment of Canada. 2015. “Geometric Distortion in Imagery.” https://natural-resources.canada.ca/maps-tools-and-publications/satellite-imagery-and-air-photos/tutorial-fundamentals-remote-sensing/satellites-and-sensors/geometric-distortion-imagery/9401.\n\n\nJensen, J R. 2015. Introductory Digital Image Processing: A Remote Sensing Perspective. 4th ed. Upper Saddle River.: Prentice Hall Press.\n\n\nJiang, Zhangyan, Alfredo Huete, Youngwook Kim, and K. Didan. 2007. “2-Band Enhanced Vegetation Index Without a Blue Band and Its Application to AVHRR Data.” Proceedings of SPIE - The International Society for Optical Engineering 6679. https://doi.org/10.1117/12.734933.\n\n\nToutin, Thierry. 2011. “State-of-the-Art of Geometric Correction of Remote Sensing Data: A Data Fusion Perspective.” International Journal of Image and Data Fusion 2 (1). https://doi.org/10.1080/19479832.2010.539188.\n\n\nTucker, Compton J, John R G Townshend, and Thomas E Goff. 1985. “African Land-Cover Classification Using Satellite Data.” Science 227 (4685). https://doi.org/10.1126/science.227.4685.369.\n\n\nUSGS. n.d.a. “Landsat Enhanced Vegetation Index.” https://www.usgs.gov/landsat-missions/landsat-enhanced-vegetation-index.\n\n\n———. n.d.b. “Landsat Normalized Difference Vegetation Index.” https://www.usgs.gov/landsat-missions/landsat-normalized-difference-vegetation-index.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "Chapter 4.html",
    "href": "Chapter 4.html",
    "title": "4  Policy",
    "section": "",
    "text": "4.1 Summary\nAccording to a CDP report on cities at risk of climate change (CDP 2023), the top five hazards that cities face include flooding, heat waves, rain storms, extreme hot days and droughts. Los Angeles is no stranger to such hazards, such as flooding. In February 2024, Los Angeles, along with other parts of California, faced a rare Level 4 of 4 risk of excessive rainfall (CNN 2024). This threat level means that the city is facing “life threatening flash and urban flash flooding”, in this case due to atmospheric rivers. In the first quarter of 2023, it was estimated that flooding caused approximately $30 billion in damages to the state of California (Cleanfax 2023).\nLos Angeles, in order to better manage its water-related resources and hazards released the One Water LA 2040 Plan in 2018 (City of Los Angeles 2018). The Metropolitan outlined seven goals of which Goal 6 of “Increas[ing] climate resilience by planning for climate change mitigation and adaptation strategies in all City actions” is related to flooding. In terms of the United Nations Sustainable Development Goal (SDG), this is related mainly to Goal 11: Sustainable Cities and Communities of “making cities and human settlements inclusive, safe, resilient and sustainable”, along with Goal 13: Climate Action to “take urgent action to combat climate change and its impacts” (United Nations, n.d.).\nThe city’s stormwater planning approach aims for projects that improve water quality, augment water supply and mitigate flood risks. These include distributed and regional green infrastructure projects, that differ based on scale. The former nature-based solutions include bioretention cells, bioswales, catch basin retrofits while the latter includes capture-storage-use systems and nature-inspired-flow-through treatment wetlands. In addition, the city has plans to implement more grey infrastructure projects such as canals and reservoirs.\nSummary of Stormwater Projects as detailed in the One Water LA 2040 Plan Summary Report. Source: City of Los Angeles (2018)\nHowever, critics argue that Los Angeles’ water management strategy overly relies on grey infrastructure to quickly flush water away, neglecting the potential benefits of green infrastructure projects to absorb and store water (The Guardian 2023) as green infrastructure are seen to be 50% more cost effective than grey infrastructure (World Economic Forum 2022). In Los Angeles, even when green infrastructure projects are proposed, there are further concerns that they may not be equitably distributed or planned in locations with the ideal conditions for groundwater infiltration (Environment and Sustainability 2022).\nTo address these challenges, remote sensing could be a valuable tool for policy planning in the placement of nature-based solutions. The overall workflow would require a stocktake of the city’s green and blue infrastructure or the ability of areas to absorb water, followed by overlaying it with flood data to identify areas to implement nature-based solutions. Further extensions can be added to ensure partnership with different stakeholders and meet other SDGs.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "Chapter 4.html#application",
    "href": "Chapter 4.html#application",
    "title": "4  Policy",
    "section": "4.2 Application",
    "text": "4.2 Application\nIn 2023, Arup released the Global Sponge Cities Snapshot report to examine how “spongy” 10 cities are which relates to their ability to harvest and store water (Arup 2023). Its methodology to produce such a snapshot involved quantifying the amount of blue, green and grey infrastructure through machine learning of satellite imagery. Arup subsequently considered the quantity of each major hydrological soil type in each city before calculating the surface runoff.\n\n\n\n\n\n\n\n\n\nArup’s methodology to calculate the city’s sponge snapshot. Source: Arup (2023)\nTo reproduce such a snapshot, the following steps are required:\nStep 1: Quantifying city’s blue, green and grey infrastructure.\nSentinel 2 or other remote sensing images can be obtained for the metropolitan area. Supervised classification or machine learning can be employed in order to classify the different land cover such as water bodies, vegetation, buildings, roads, etc.\n\n\n\n\n\n\n\n\n\nTimelapse of Sentinel 2 images of Los Angeles. Source: Sentinel Hub (n.d.)\nStep 2: Soil types and vegetation\nThe next step would be to factor in the soil type which would influence ground permeability and porosity. Hyperspectral imaging of the topsoil, combined with regression modelling, can be used to estimate the percentage of sand and clay in a bare field (Ewing et al. 2020). This can be done through the use of the visible and near-infrared bands to create a soil moisture map.\nStep 3: Water runoff potential\nA digital elevation model (DEM) can be used to generate the slope profile for Los Angeles. Through combining with land use and soil type, the water runoff potential for the area can be estimated (Mahmoud 2014).\nThrough Steps (1) to (3), by quantifying the city’s blue, green and grey infrastructure, factoring in the soil type and vegetation and water runoff potential, an index or map of the city’s ability to absorb and store water can be obtained. With the map, the idea is for authorities to understand the city’s profile and make better informed decisions in the implementation of nature-based solutions (Arup 2023). As an extension of Arup’s sponge snapshot, the following are other considerations to enhance authorities’ efforts in dealing with floods.\nConsideration A: Flood maps and modelling\nTo obtain accurate and up-to-date flood maps, the use of synthetic aperture radar (SAR) images can be used (Brivio et al. 2002). Even if the satellite overpass may not coincide with the flood peak, integrating the map of inundated areas obtained through SAR with digital topographic data can help estimate flooded areas. Through overlaying the flood maps with the city’s sponge snapshot, authorities are able to identify areas affected severely by floods and with low capacity to store water as a priority area for intervention. Flood models can be run to examine the effect of placing these solutions. Alternatively, authorities are able to identify if there are any best practices in land use with regards to flood managements. This could possibly include large stretches of nature spaces that are able to hold water without inundating the surrounding areas.\nConsideration B: Local efforts to mitigate floods\nHomeowners in flood prone areas may take measures to floodproof their house such as elevating or dry flood-proofing their houses (Botzen et al. 2019). It is possible that homeowners could undertake unauthorised or unpermitted changes in flood infrastructure such as constructing new concrete structure or encroachments into floodplains that may compromise authorities’ efforts. Change detection, by comparing satellite images of different time periods will allow authorities to monitor if any such development is occurring and react accordingly.\nConsideration C: Planning for connectivity, accessibility and liveability through space syntax\nIn addition to the SDG goals mentioned about sustainable cities and communities, and climate action, there is a potential to integrate the SDG goal of good health and well-being. Flood maps can be combined with spatial analysis techniques such as space syntax to provide insights into the connectivity, accessibility and spatial structure of urban area (Dursun 2007). Through integrating this data, authorities can ensure that the placement of nature-based solutions not only mitigate flooding but also contributing to creating more liveable environments such as more parks for people to enhance their quality of life.\nOverall, the workflow presented above can aid authorities in site selection for nature-based solutions based on mitigation needs, while also considering other possible stakeholders and positive outcomes that can be accomplished as well.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "Chapter 4.html#reflection",
    "href": "Chapter 4.html#reflection",
    "title": "4  Policy",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nThe above steps based on Arup’s report shows how remote sensing can be used to aid policymakers in creating a comprehensive map to make well-informed decisions on the areas most vulnerable to floods, best practices of areas that are able to mitigate flood effects, and possible areas to implement nature-based solutions. The addition of three other considerations shows that flooding is a complex issue with many other interconnected dimensions. This suggests that while remote sensing is valuable for policymakers, it is not a magic bullet. Stakeholders at different scales must be considered. Furthermore, for any measure implemented, it is essential to consider potential repercussions as well as possible opportunities such as liveability to integrate. Remote sensing is also not a one-off tool and can be used to periodically monitor the situation and effectiveness of measures implemented.\nAlthough remote sensing can be informative, it is important to recognise there may be challenges that limits its usage in policy. Challenges include the complexity of processing data, analysing results and communication findings, which may be more difficult for people to comprehend compared to simple charts and tables. To increase its use in policy, more collaboration and coordination among data providers, researchers, authorities and the general public is needed to harness it better.\n\n\n\n\nArup. 2023. “Arup Global Sponge City Snapshot.” https://www.arup.com/perspectives/publications/research/section/global-sponge-cities-snapshot.\n\n\nBotzen, W J Wouter, Howard Kunreuther, Jeffrey Czajkowski, and Hans de Moel. 2019. “Adoption of Individual Flood Damage Mitigation Measures in New York City: An Extension of Protection Motivation Theory.” Risk Analysis 39 (10): 2143–59. https://doi.org/10.1111/risa.13318.\n\n\nBrivio, P A, R Colombo, M Maggi, and R Tomasoni. 2002. “Integration of Remote Sensing Data and GIS for Accurate Mapping of Flooded Areas.” International Journal of Remote Sensing 23 (3): 429–41. https://doi.org/10.1080/01431160010014729.\n\n\nCDP. 2023. “Cities at Risk.” https://www.cdp.net/en/research/global-reports/cities-at-risk.\n\n\nCity of Los Angeles. 2018. “One Water LA 2040 Plan.” https://www.lacitysan.org/cs/groups/sg_owla/documents/document/y250/mdmx/~edisp/cnt031540.pdf.\n\n\nCleanfax. 2023. “California Flooding Damage Estimates Top $30B.” https://cleanfax.com/california-flooding-damage-estimates-top-30b/.\n\n\nCNN. 2024. “More Than 800,000 Without Power in California as Intense Atmospheric River Brings Threat of Mudslides and Flooding.” https://edition.cnn.com/2024/02/04/us/california-atmospheric-river-flooding/index.html.\n\n\nDursun, Pelin. 2007. “6th International Space Syntax Symposium.” In. Istanbul.\n\n\nEnvironment, UCLA Institute of the, and Sustainability. 2022. “Evaluating Equity Under Measure w  Los Angeles County’s Safe Clean Water Program.” https://www.ioes.ucla.edu/project/evaluating-equity-under-measure-w-los-angeles-countys-safe-clean-water-program/.\n\n\nEwing, Jordan, Thomas Oommen, Paramsothy Jayakumar, and Russell Alger. 2020. “Utilizing Hyperspectral Remote Sensing for Soil Gradation.” Remote Sensing 12 (20). https://doi.org/10.3390/rs12203312.\n\n\nMahmoud, Shereif H. 2014. “Investigation of Rainfallrunoff Modeling for Egypt by Using Remote Sensing and GIS Integration.” CATENA 120: 111–21. https://doi.org/10.1016/j.catena.2014.04.011.\n\n\nSentinel Hub. n.d. “Sentinel Hub EO Browser.” https://apps.sentinel-hub.com/eo-browser/?zoom=10&lat=41.9&lng=12.5&themeId=DEFAULT-THEME&toTime=2024-02-06T19%3A54%3A24.499Z.\n\n\nThe Guardian. 2023. “The Parched Metropolis: Can Eco Architecture Save LA from Megadrought?” https://www.theguardian.com/artanddesign/2023/feb/27/parched-eco-architecture-los-angeles-megadrought-water-capturing-parks.\n\n\nUnited Nations. n.d. “The 17 Goals.” https://sdgs.un.org/goals.\n\n\nWorld Economic Forum. 2022. “Nearly Half of City GDP at Risk of Disruption from Nature Loss, New Report Finds.” https://www.weforum.org/press/2022/01/biodivercities-initiative-set-to-transform-global-urban-infrastructure-by-2030/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ahady, Abdul Baqi, and Gordana Kaplan. 2022. “Classification\nComparison of Landsat-8 and Sentinel-2 Data in\nGoogle Earth Engine, Study Case of the City of\nKabul.” International Journal of Engineering and\nGeosciences 7 (1): 24–31. https://doi.org/10.26833/ijeg.860077.\n\n\nAl-amri, Salem Saleh, N. V. Kalyankar, and S. D. Khamitkar. 2010.\n“Contrast Stretching Enhancement in Remote Sensing Image.”\nInternational Journal of Computer Science Issues (IJCSI) 7 (2):\n26–29.\n\n\nAlaska Satellite Facility. 2019a. “What Is SAR?” https://asf.alaska.edu/information/sar-information/what-is-sar/.\n\n\n———. 2019b. “How Do i Interpret SAR Images?” https://asf.alaska.edu/information/sar-information/how-do-i-read-sar-images/.\n\n\nArup. 2023. “Arup Global Sponge City Snapshot.” https://www.arup.com/perspectives/publications/research/section/global-sponge-cities-snapshot.\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma\nKilpi. 2019. “Comparison of Sentinel-2 and\nLandsat 8 Imagery for Forest Variable Prediction in Boreal\nRegion.” Remote Sensing of Environment 223: 257–73. https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nBakker, Jonathan D. 2015. “Overview of Classification and\nRegression Trees.” In. Switzerland: Springer. https://uw.pressbooks.pub/appliedmultivariatestatistics/chapter/classification-and-regression-trees/.\n\n\nBallinger, Ollie. 2024. “CASA0025: Building Spatial Applications\nwith Big Data.” https://oballinger.github.io/CASA0025/.\n\n\nBasith, Abdul. 2011. “Landslide Susceptibility Modelling Under\nEnvironmental Changes: A Case Study of Cameron Highlands,\nMalaysia.” PhD thesis, Perak. https://www.researchgate.net/publication/320710942_LANDSLIDE_SUSCEPTIBILITY_MODELLING_UNDER_ENVIRONMENTAL_CHANGES_A_CASE_STUDY_OF_CAMERON_HIGHLANDS_MALAYSIA.\n\n\nBelgiu, Mariana, and Lucian Drăguţ. 2016. “Random Forest in Remote\nSensing: A Review of Applications and Future Directions.”\nISPRS Journal of Photogrammetry and Remote Sensing 114: 24–31.\nhttps://doi.org/10.1016/j.isprsjprs.2016.01.011.\n\n\nBoroughani, Mahdi, Sima Pourhashemi, Hamid Gholami, and Dimitris G\nKaskaoutis. 2021. “Predicting of Dust Storm Source by Combining\nRemote Sensing, Statistic-Based Predictive Models and Game Theory in the\nSistan Watershed, Southwestern Asia.”\nJournal of Arid Land 13: 1103–21. https://doi.org/10.1007/s40333-021-0023-3.\n\n\nBotzen, W J Wouter, Howard Kunreuther, Jeffrey Czajkowski, and Hans de\nMoel. 2019. “Adoption of Individual Flood Damage Mitigation\nMeasures in New York City: An Extension of Protection Motivation\nTheory.” Risk Analysis 39 (10): 2143–59. https://doi.org/10.1111/risa.13318.\n\n\nBrivio, P A, R Colombo, M Maggi, and R Tomasoni. 2002.\n“Integration of Remote Sensing Data and GIS for Accurate Mapping\nof Flooded Areas.” International Journal of Remote\nSensing 23 (3): 429–41. https://doi.org/10.1080/01431160010014729.\n\n\nBrownlee, Jason. 2020. “Classification and Regression Trees for\nMachine Learning.” https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/.\n\n\nCalifornia Institute of Technology. n.d. “Overview | Get to Know\nSAR.” https://nisar.jpl.nasa.gov/mission/get-to-know-sar/overview.\n\n\nCarbonBrief. 2019. “Explainer: The High-Emissions\n‘RCP8.5’ Global Warming Scenario.” https://www.carbonbrief.org/explainer-the-high-emissions-rcp8-5-global-warming-scenario/.\n\n\nCastaldi, Fabio. 2021. “Sentinel-2 and Landsat-8\nMulti-Temporal Series to Estimate Topsoil Properties\non Croplands.” Remote Sensing 13 (17). https://doi.org/10.3390/rs13173345.\n\n\nCDP. 2023. “Cities at Risk.” https://www.cdp.net/en/research/global-reports/cities-at-risk.\n\n\nChen, Guandong, Yu Li, Guangmin Sun, and Yuanzhi Zhang. 2017.\n“Application of Deep Networks to Oil Spill Detection Using\nPolarimetric Synthetic Aperture Radar Images.” Applied\nSciences 7 (10): 968. https://doi.org/10.3390/app7100968.\n\n\nCIESIN. 1993. “Box 4-b–System Tradeoffs.” http://www.ciesin.org/docs/005-356/box4B.html#:~:tet=Sensor%20design%20requires%20tradeoffs%20among,at%20the%20expense%20of%20another.\n\n\nCity of Los Angeles. 2018. “One Water LA 2040 Plan.” https://www.lacitysan.org/cs/groups/sg_owla/documents/document/y250/mdmx/~edisp/cnt031540.pdf.\n\n\nCleanfax. 2023. “California Flooding Damage Estimates Top\n$30B.” https://cleanfax.com/california-flooding-damage-estimates-top-30b/.\n\n\nCNN. 2024. “More Than 800,000 Without Power in California as\nIntense Atmospheric River Brings Threat of Mudslides and\nFlooding.” https://edition.cnn.com/2024/02/04/us/california-atmospheric-river-flooding/index.html.\n\n\nDu, Peijun, Sicong Liu, Pei Liu, Kun Tan, and Liang CHENG. 2014.\n“Sub-Pixel Change Detection for Urban Land-Cover Analysis via\nMulti-Temporal Remote Sensing Images.” Geo-Spatial\nInformation Science 17 (1): 26–38. https://doi.org/10.1080/10095020.2014.889268.\n\n\nDursun, Pelin. 2007. “6th International Space Syntax\nSymposium.” In. Istanbul.\n\n\nEarth, Penn State College of, and Mineral Science. n.d. “14. Image\nCorrection.” https://www.e-education.psu.edu/natureofgeoinfo/node/1896.\n\n\nEnvironment, UCLA Institute of the, and Sustainability. 2022.\n“Evaluating Equity Under Measure w  Los Angeles\nCounty’s Safe Clean Water Program.” https://www.ioes.ucla.edu/project/evaluating-equity-under-measure-w-los-angeles-countys-safe-clean-water-program/.\n\n\nEsri. n.d. “Accuracy Assessment.” https://pro.arcgis.com/en/pro-app/3.1/help/analysis/image-analyst/accuracy-assessment.htm.\n\n\nEwing, Jordan, Thomas Oommen, Paramsothy Jayakumar, and Russell Alger.\n2020. “Utilizing Hyperspectral Remote Sensing for Soil\nGradation.” Remote Sensing 12 (20). https://doi.org/10.3390/rs12203312.\n\n\nFoody, Giles M. 2020. “Explaining the Unsuitability of the Kappa\nCoefficient in the Assessment and Comparison of the Accuracy of Thematic\nMaps Obtained by Image Classification.” Remote Sensing of\nEnvironment 239: 111630. https://doi.org/10.1016/j.rse.2019.111630.\n\n\nGeeksforGeeks. 2023. “CART (Classification and Regression Tree) in\nMachine Learning.” https://www.geeksforgeeks.org/cart-classification-and-regression-tree-in-machine-learning/.\n\n\nGhrefat, Habes, Muheeb Awawdeh, Fares Howari, and Abdulla Al-Rawabdeh.\n2023. “Chapter 12 - Mineral Exploration Using Multispectral and\nHyperspectral Remote Sensing Data.” In, edited by Nikolaos\nStathopoulos, Andreas Tsatsaris, and Kleomenis Kalogeropoulos, 197–222.\nEarth Observation. Elsevier.\n\n\nGibson, Rebecca, Tim Danaher, Warwick Hehir, and Luke Collins. 2020.\n“A Remote Sensing Approach to Mapping Fire Severity in\nSouth-Eastern Australia Using Sentinel 2 and Random Forest.”\nRemote Sensing of Environment 240: 111702. https://doi.org/10.1016/j.rse.2020.111702.\n\n\nGIS Geography. 2015. “Passive Vs Active Sensors in Remote\nSensing.” https://gisgeography.com/passive-active-sensors-remote-sensing/.\n\n\n———. 2023. “What Is Remote Sensing? The Definitive Guide.”\nhttps://gisgeography.com/remote-sensing-earth-observation-guide/.\n\n\nGoogle Earth Engine. n.d.a. “Ee.image.glcmTexture.” https://developers.google.com/earth-engine/apidocs/ee-image-glcmtexture.\n\n\n———. n.d.b. “Google Earth Engine.” https://earthengine.google.com/.\n\n\n———. n.d.c. “Linear Regression.” https://developers.google.com/earth-engine/guides/reducers_regression.\n\n\nGovernment of Canada. 2015. “Geometric Distortion in\nImagery.” https://natural-resources.canada.ca/maps-tools-and-publications/satellite-imagery-and-air-photos/tutorial-fundamentals-remote-sensing/satellites-and-sensors/geometric-distortion-imagery/9401.\n\n\nHe, Mingzhu, John S. Kimball, Marco P. Maneta, Bruce D. Maxwell, Alvaro\nMoreno, Santiago Beguería, and Xiaocui Wu. 2018. “Regional Crop\nGross Primary Productivity and Yield Estimation Using Fused\nLandsat-MODIS Data.” Remote Sensing 10 (3): 372. https://doi.org/10.3390/rs10030372.\n\n\nInside Learning Machines. 2023. “8 Key Advantages and\nDisadvantages of Decision Trees.” https://insidelearningmachines.com/advantages_and_disadvantages_of_decision_trees/.\n\n\nInstitute of Physics. n.d. “Different Orbits.” https://spark.iop.org/different-orbits.\n\n\nJensen, J R. 2015. Introductory Digital Image Processing: A Remote\nSensing Perspective. 4th ed. Upper Saddle River.: Prentice Hall\nPress.\n\n\nJia, Jianxin, Jinsong Chen, Xiaorou Zheng, Yueming Wang, Shanxin Guo,\nHaibin Sun, Changhui Jiang, et al. 2022. “Tradeoffs in the\nSpatial and Spectral Resolution of\nAirborne Hyperspectral Imaging Systems: A Crop\nIdentification Case Study.” IEEE Transactions on\nGeoscience and Remote Sensing 60: 1–18. https://doi.org/10.1109/TGRS.2021.3096999.\n\n\nJiang, Zhangyan, Alfredo Huete, Youngwook Kim, and K. Didan. 2007.\n“2-Band Enhanced Vegetation Index Without a Blue Band and Its\nApplication to AVHRR Data.” Proceedings of SPIE - The\nInternational Society for Optical Engineering 6679. https://doi.org/10.1117/12.734933.\n\n\nKarabiber, Faith. n.d. “Gini Impurity.” https://www.learndatasci.com/glossary/gini-impurity/.\n\n\nKarasiak, N., J.-F. Dejoux, C. Monteil, and D. Sheeren. 2022.\n“Spatial Dependence Between Training and Test Sets: Another\nPitfall of Classification Accuracy Assessment in Remote Sensing.”\nMachine Learning 111 (7): 2715–40. https://doi.org/10.1007/s10994-021-05972-1.\n\n\nLi, Shenglin, Jinglei Wang, Dacheng Li, Zhongxin Ran, and Bo Yang. 2021.\n“Evaluation of Landsat 8-Like Land Surface\nTemperature by Fusing Landsat 8 and MODIS Land\nSurface Temperature Product.” Processes 9 (12).\nhttps://doi.org/10.3390/pr9122262.\n\n\nLiang, Shunlin, and Jindi Wang. 2020. Advanced Remote Sensing -\nTerrestrial Information Extraction and Applications. 2nd ed.\nLondon: Elsevier.\n\n\nLing, Feng, Doreen Boyd, Yong Ge, Giles M. Foody, Xiaodong Li, Lihui\nWang, Yihang Zhang, et al. 2019. “Measuring River Wetted Width\nfrom Remotely Sensed Imagery at the Subpixel Scale with a Deep\nConvolutional Neural Network.” Water Resources Research\n55 (7): 5631–49. https://doi.org/10.1029/2018WR024136.\n\n\nLópez-Puigdollers, Dan, Gonzalo Mateo-Garcia, and Luis Gómez-Chova.\n2021. “Benchmarking Deep Learning Models for\nCloud Detection in Landsat-8 and\nSentinel-2 Images.” Remote Sensing 13 (5).\nhttps://doi.org/10.3390/rs13050992.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. “12\nStatistical Learning.” In, 1st ed. London: Chapman & Hall. https://r.geocompx.org/spatial-cv.html.\n\n\nMacLachlan, Andy. 2024. “CASA0023 Remotely Sensing Cities and\nEnvironments.” https://andrewmaclachlan.github.io/CASA0023/.\n\n\nMahmoud, Shereif H. 2014. “Investigation of\nRainfallrunoff Modeling for Egypt by Using Remote Sensing\nand GIS Integration.” CATENA 120: 111–21. https://doi.org/10.1016/j.catena.2014.04.011.\n\n\nMazza, Antonio, Massimiliano Gargiulo, Giuseppe Scarpa, and Raffaele\nGaetano. 2018. “Estimating the NDVI from SAR by Convolutional\nNeural Networks.” In, 1954–57. https://doi.org/10.1109/IGARSS.2018.8519459.\n\n\nMinitab. n.d. “Model Summary for CART® Regression.” https://support.minitab.com/en-us/minitab/20/help-and-how-to/statistical-modeling/predictive-analytics/how-to/cart-regression/interpret-the-results/all-statistics-and-graphs/model-summary-table/.\n\n\nMutanga, Onisimo, and Lalit Kumar. 2019. “Google Earth Engine\nApplications.” Remote Sensing 11 (5): 591. https://doi.org/10.3390/rs11050591.\n\n\nNASA. 2020a. “What Is Synthetic Aperture Radar? |\nEarthdata.” https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\n———. 2020b. “Scientists Map Beirut Blast Damage.” https://earthobservatory.nasa.gov/images/147098/scientists-map-beirut-blast-damage.\n\n\n———. n.d. “What Is Remote Sensing?” https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\nOrusa, T, and E Borgogno Mondino. 2019. “Landsat 8 Thermal Data to\nSupport Urban Management and Planning in the Climate Change Era: A Case\nStudy in Torino Area, NW Italy.”\nRemote Sensing Technologies and Applications in Urban Environments\nIV 11157. https://doi.org/10.1117/12.2533110.\n\n\nPark, Seonyoung, Eunkyo Seo, Daehyun Kang, Jungho Im, and Myong-In Lee.\n2018. “Prediction of Drought on Pentad Scale Using Remote Sensing\nData and MJO Index Through Random Forest over East Asia.”\nRemote Sensing 10 (11): 1811. https://doi.org/10.3390/rs10111811.\n\n\nSchaefer, Lauren N., Zhong Lu, and Thomas Oommen. 2016.\n“Post-Eruption Deformation Processes Measured Using ALOS-1 and\nUAVSAR InSAR at Pacaya Volcano, Guatemala.” Remote\nSensing 8 (1): 73. https://doi.org/10.3390/rs8010073.\n\n\nSegarra, Joel, Maria Luisa Buchaillot, Jose Luis Araus, and Shawn C.\nKefauver. 2020. “Remote Sensing for Precision\nAgriculture: Sentinel-2 Improved Features and\nApplications.” Agronomy 10 (5). https://doi.org/10.3390/agronomy10050641.\n\n\nSentinel Hub. n.d. “Sentinel Hub EO Browser.” https://apps.sentinel-hub.com/eo-browser/?zoom=10&lat=41.9&lng=12.5&themeId=DEFAULT-THEME&toTime=2024-02-06T19%3A54%3A24.499Z.\n\n\nSentinel Online. n.d. “Resolution and Swath.” https://sentinel.esa.int/web/sentinel/missions/sentinel-2/instrument-payload/resolution-and-swath.\n\n\nSharma, Richa, Aniruddha Ghosh, and P. K. JOSHI. 2013. “Decision\nTree Approach for Classification of Remotely Sensed Satellite Data Using\nOpen Source Support.” Journal of Earth System Science\n122 (5): 1237–47. https://doi.org/10.1007/s12040-013-0339-2.\n\n\nSuresh, M., and Kamal Jain. 2018. “Subpixel Level Mapping of\nRemotely Sensed Image Using Colorimetry.” The Egyptian\nJournal of Remote Sensing and Space Science 21 (1): 65–72. https://doi.org/10.1016/j.ejrs.2017.02.004.\n\n\nThe Guardian. 2023. “The Parched Metropolis: Can Eco Architecture\nSave LA from Megadrought?” https://www.theguardian.com/artanddesign/2023/feb/27/parched-eco-architecture-los-angeles-megadrought-water-capturing-parks.\n\n\nTheVerge. 2023. “How Google Earth Engine Revolutionized the Way We\nMonitor Deforestation.” https://www.theverge.com/23746844/google-earth-engine-amazon-deforestation-monitoring.\n\n\nToutin, Thierry. 2011. “State-of-the-Art of Geometric Correction\nof Remote Sensing Data: A Data Fusion Perspective.”\nInternational Journal of Image and Data Fusion 2 (1). https://doi.org/10.1080/19479832.2010.539188.\n\n\nTucker, Compton J, John R G Townshend, and Thomas E Goff. 1985.\n“African Land-Cover Classification Using Satellite Data.”\nScience 227 (4685). https://doi.org/10.1126/science.227.4685.369.\n\n\nUm, Albert. 2021. “Introduction to Sentinel 1: SAR\n(Synthetic-Aperture Radar) Data.” https://albertum.medium.com/introduction-to-sentinel-1-sar-synthetic-aperture-radar-data-2cca22cb35a6.\n\n\nUnited Nations. n.d. “The 17 Goals.” https://sdgs.un.org/goals.\n\n\nUSGS. n.d.a. “InSARsatellite-Based Technique Captures\nOverall Deformation \"Picture\".” https://www.usgs.gov/programs/VHP/insar-satellite-based-technique-captures-overall-deformation-picture.\n\n\n———. n.d.b. “Landsat Enhanced Vegetation Index.” https://www.usgs.gov/landsat-missions/landsat-enhanced-vegetation-index.\n\n\n———. n.d.c. “Landsat Normalized Difference Vegetation\nIndex.” https://www.usgs.gov/landsat-missions/landsat-normalized-difference-vegetation-index.\n\n\n———. n.d.d. “What Is Remote Sensing and What Is It Used\nFor?” https://www.usgs.gov/faqs/what-remote-sensing-and-what-it-used.\n\n\nWilber, Jared. 2022. “Precision and Recall.” https://mlu-explain.github.io/precision-recall/.\n\n\nWilkinson, R, M M Mleczko, R J W Brewin, K J Gaston, M Mueller, J D\nShutler, X Yan, and K Anderson. 2024. “Environmental Impacts of\nEarth Observation Data in the Constellation and Cloud Computing\nEra.” Science of The Total Environment 909. https://doi.org/10.1016/j.scitotenv.2023.168584.\n\n\nWorld Economic Forum. 2022. “Nearly Half of City GDP at Risk of\nDisruption from Nature Loss, New Report Finds.” https://www.weforum.org/press/2022/01/biodivercities-initiative-set-to-transform-global-urban-infrastructure-by-2030/.\n\n\nWu, Zhiwei, Hong He, Yu Liang, Longyan Cai, and Bernard Lewis. 2013.\n“Determining Relative Contributions of Vegetation and Topography\nto Burn Severity from LANDSAT Imagery.” Environmental\nManagement 52. https://doi.org/10.1007/s00267-013-0128-3.\n\n\nYadav, Ajay. 2019. “Decision Trees.” https://towardsdatascience.com/decision-trees-d07e0f420175.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "Chapter 1.html",
    "href": "Chapter 1.html",
    "title": "1  Getting started with remote sensing",
    "section": "",
    "text": "1.1 Summary\nIn the first lesson of the course, we covered the basics of remote sensing. These include the definition of remote sensing, types of remote sensing, electromagnetic signatures and types of resolution. The summary section will mainly focus on the types of resolution.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started with remote sensing</span>"
    ]
  },
  {
    "objectID": "Chapter 1.html#summary",
    "href": "Chapter 1.html#summary",
    "title": "1  Getting started with remote sensing",
    "section": "",
    "text": "1.1.1 Definition of Remote Sensing\nUSGS: Remote Sensing is the process of detecting and monitoring the physical characteristics of an area by measuring its reflected and emitted radiation at a distance (typically from satellite or aircraft) (USGS, n.d.).\nNASA: Remote sensing is acquiring of information from a distance. NASA observes Earth and other planetary bodies via remote sensors on satellites and aircraft that detect and record reflected or emitted energy (NASA, n.d.).\n\n\n1.1.2 Types of Resolution\n\nSpatial Resolution: “the size of each pixel within a digital image and the area on Earth’s surface represented by that pixel” (NASA, n.d.). A finer spatial resolution (or the smaller the number) means one is able to observe greater details while a coarser spatial resolution (or the bigger the number) means the image would be more pixelated as demonstrated below.\n\n\n\n\n\n\n\n\n\n\nCampus of the University of Maryland at College Park at 1m, 10m, 30m and 250m spatial resolution. Source: Liang and Wang (2020)\n\nTemporal Resolution: frequency at which a satellite revisits the same observation area (NASA, n.d.). Geostationary satellites orbit at the same rate of the earth and focus on the same observation area resulting in a higher temporal resolution. In contrast, polar orbiting satellites orbit around the entire Earth and able to observe the different parts of the world, at the cost of temporal resolution which could vary from 1 to 16 days.\n\n\n\n\n\n\n\n\n\n\nGeostationary and Polar Orbiting Satellites. Source: Institute of Physics (n.d.)\n\nSpectral Resolution: “the number and width of spectral bands in a sensor system” (Liang and Wang 2020). Sensors which have more bands with narrower wavelengths, have finer spectral resolution. This would allow greater distinction to be made between various features such as rock or vegetation types.\n\n\n\n\n\n\n\n\n\n\nComparison of spectral bands between Sentinel-2 and Landsat-8 which have similar characteristics for the coastal aerosol (440 nm), visible (490-800 nm) and short-wave infrared or SWIR (1300-2400 nm) spectral bands. Source: López-Puigdollers, Mateo-Garcia, and Gómez-Chova (2021)\n\nRadiometric Resolution: “amount of information in each pixel” (NASA, n.d.). Sensors with higher radiometric resolution are more sensitive and able to detect subtle differences in electromagnetic energy. This allows for features to be distinguished better such as variations in vegetation or water colour.\n\n\n\n\n\n\n\n\n\n\nCampus of the University of Maryland at College Park at (A) 8 bits (28 or 256 discrete shades of grey) , (B) 4 bits (24 or 16 discrete shades of grey), (C) 2 bits (22 or 4 discrete shades of grey) and (D) 1 bit (21 or 2 discrete shades of grey) radiometric resolution. Source: Liang and Wang (2020)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started with remote sensing</span>"
    ]
  },
  {
    "objectID": "Chapter 1.html#application",
    "href": "Chapter 1.html#application",
    "title": "1  Getting started with remote sensing",
    "section": "1.2 Application",
    "text": "1.2 Application\n\n1.2.1 Practical Extracts\nAs part of our first practical for remote sensing, we had to select a city of our choice (Los Angeles) and source for Sentinel 2 and Landsat 8 images related to it. In SNAP, I selected various points of interest (urban, high urban, water, vegetation and mountains).\n\n\n\n\n\n\n\n\n\nLandsat 8 image of Los Angeles with Points of Interest selected in SNAP. Source: Author’s Own.\nR was then used to plot the spectral profiles for both Sentinel 2 and Landsat 8. The results are as follow.\n\n\n\n\n\n\n\n\n\nSpectral Profile generated from Sentinel 2. Source: Author’s Own.\n\n\n\n\n\n\n\n\n\nSpectral Profile generated from Landsat 8. Source: Author’s Own.\n\n\n1.2.2 Sentinel 2 or Landsat 8. Which one should I use?\n\n\n\n\n\n\n\n\n\nComparison of Landsat 8 vs. Sentinel 2 bands. Source: Ahady and Kaplan (2022)\nSentinel 2 performs better than Landsat 8 as it has better spatial resolution as seen from the pixel size column in the table above (Ahady and Kaplan 2022). It also has a better temporal resolution of ten days at the equator using one satellite, and five days using two satellites, under the assumption of cloud-free conditions (Sentinel Online, n.d.). In contrast, Landsat 8 has a temporal resolution of 16 days (Castaldi 2021).\nA study of both satellites in forest variable prediction shows that Sentinel 2 performed better than Landsat 8 in predictive accuracy, due to the former’s higher spatial resolution and additional red-edge bands (Astola et al. 2019). Sentinel 2 has four narrow bands in the red edge spectral domain which allows it to accurately study vegetation health or crop stress monitoring (Segarra et al. 2020).\nWhere Landsat 8 performs better is that it has thermal infrared sensors to measure surface temperature, a feature that Sentinel 2 does not possess (USGS, n.d.). This makes Landsat 8 suitable for studying processes such as the hydrologic cycle and climate change (Li et al. 2021). Landsat 8’s capabilities to measure land surface temperature also enables it to identify areas where the urban heat island effect is strongest in cities (Orusa and Mondino 2019).\nOverall, Sentinel-2 would be useful for looking at Land Use Land Cover (LULC) and vegetation related studies while Landsat 8 would be useful for studies related to urban heat island and climate change.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started with remote sensing</span>"
    ]
  },
  {
    "objectID": "Chapter 1.html#reflection",
    "href": "Chapter 1.html#reflection",
    "title": "1  Getting started with remote sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nWhy can’t we develop a satellite that boast the best spatial, temporal, spectral and radiometric resolution, while also incorporating sensors for diverse measurements such as temperature? The limitations stem from inherent trade-offs related to the sensor designs. For example, better quality spatial and multispectral data on satellites would compete for onboard data storage space, as well as data transmission requirements to communicate with ground stations (CIESIN 1993). To improve the signal-to-noise ratio, solutions such as having a larger pixel size or accepting light from a wider spectral interval can be used, but will result in a decrease in spatial and spectral resolution respectively (Jia et al. 2022).\nGiven that no perfect satellite exists currently, it ultimately depends on one’s research needs and what to prioritise. MODIS which has 36 bands, can be used to study dust clouds because its bands 29-32 are able to track dust aerosols and it has a temporal resolution of one day (Boroughani et al. 2021). However, it has medium spatial resolution at 250m, 500m and 1000m depending on the band. On the other hand, a study of land use changes over time probably does not require daily satellite images but perhaps at a monthly interval. However, higher spatial resolution to distinguish finer details to observe changes in the pattern would be essential. A table of some commonly used satellite sensors as below compiled shows some of the tradeoffs between the various resolutions.\n\n\n\n\n\n\n\n\n\nOverview of some commonly used satellite sensors. Source: Liang and Wang (2020)\nAnother thing that struck me is how big the remote sensing data is. Processing and saving in SNAP, followed by running the spectral signatures in R took quite a fair bit of waiting time as a result. I had flashbacks to my undergraduate days waiting in the lab for some remote sensing results to process on ENVI and ArcGIS. Hence, the Google Earth Engine segment would be something to look forward to given it runs on cloud computing resources and will be able to handle geospatial data compared to my personal device. I was also reminded of the recent AGI GeoCom 2023 on the ‘Environmental Impacts of Earth Observation in the Constellation and Cloud Computing Era’ presentation (Wilkinson et al. 2024). The central idea was that considering the substantial file size of remote sensing data and the fact that not all data is essential, it may be worthwhile to reassess the necessity of storing all available datasets. This is especially so due to the environmental implications associated with the storage of large datasets in data centers.\n\n\n\n\nAhady, Abdul Baqi, and Gordana Kaplan. 2022. “Classification Comparison of Landsat-8 and Sentinel-2 Data in Google Earth Engine, Study Case of the City of Kabul.” International Journal of Engineering and Geosciences 7 (1): 24–31. https://doi.org/10.26833/ijeg.860077.\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma Kilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for Forest Variable Prediction in Boreal Region.” Remote Sensing of Environment 223: 257–73. https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nBoroughani, Mahdi, Sima Pourhashemi, Hamid Gholami, and Dimitris G Kaskaoutis. 2021. “Predicting of Dust Storm Source by Combining Remote Sensing, Statistic-Based Predictive Models and Game Theory in the Sistan Watershed, Southwestern Asia.” Journal of Arid Land 13: 1103–21. https://doi.org/10.1007/s40333-021-0023-3.\n\n\nCastaldi, Fabio. 2021. “Sentinel-2 and Landsat-8 Multi-Temporal Series to Estimate Topsoil Properties on Croplands.” Remote Sensing 13 (17). https://doi.org/10.3390/rs13173345.\n\n\nCIESIN. 1993. “Box 4-b–System Tradeoffs.” http://www.ciesin.org/docs/005-356/box4B.html#:~:tet=Sensor%20design%20requires%20tradeoffs%20among,at%20the%20expense%20of%20another.\n\n\nGIS Geography. 2023. “What Is Remote Sensing? The Definitive Guide.” https://gisgeography.com/remote-sensing-earth-observation-guide/.\n\n\nInstitute of Physics. n.d. “Different Orbits.” https://spark.iop.org/different-orbits.\n\n\nJia, Jianxin, Jinsong Chen, Xiaorou Zheng, Yueming Wang, Shanxin Guo, Haibin Sun, Changhui Jiang, et al. 2022. “Tradeoffs in the Spatial and Spectral Resolution of Airborne Hyperspectral Imaging Systems: A Crop Identification Case Study.” IEEE Transactions on Geoscience and Remote Sensing 60: 1–18. https://doi.org/10.1109/TGRS.2021.3096999.\n\n\nLi, Shenglin, Jinglei Wang, Dacheng Li, Zhongxin Ran, and Bo Yang. 2021. “Evaluation of Landsat 8-Like Land Surface Temperature by Fusing Landsat 8 and MODIS Land Surface Temperature Product.” Processes 9 (12). https://doi.org/10.3390/pr9122262.\n\n\nLiang, Shunlin, and Jindi Wang. 2020. Advanced Remote Sensing - Terrestrial Information Extraction and Applications. 2nd ed. London: Elsevier.\n\n\nLópez-Puigdollers, Dan, Gonzalo Mateo-Garcia, and Luis Gómez-Chova. 2021. “Benchmarking Deep Learning Models for Cloud Detection in Landsat-8 and Sentinel-2 Images.” Remote Sensing 13 (5). https://doi.org/10.3390/rs13050992.\n\n\nNASA. n.d. “What Is Remote Sensing?” https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\nOrusa, T, and E Borgogno Mondino. 2019. “Landsat 8 Thermal Data to Support Urban Management and Planning in the Climate Change Era: A Case Study in Torino Area, NW Italy.” Remote Sensing Technologies and Applications in Urban Environments IV 11157. https://doi.org/10.1117/12.2533110.\n\n\nSegarra, Joel, Maria Luisa Buchaillot, Jose Luis Araus, and Shawn C. Kefauver. 2020. “Remote Sensing for Precision Agriculture: Sentinel-2 Improved Features and Applications.” Agronomy 10 (5). https://doi.org/10.3390/agronomy10050641.\n\n\nSentinel Online. n.d. “Resolution and Swath.” https://sentinel.esa.int/web/sentinel/missions/sentinel-2/instrument-payload/resolution-and-swath.\n\n\nUSGS. n.d. “What Is Remote Sensing and What Is It Used For?” https://www.usgs.gov/faqs/what-remote-sensing-and-what-it-used.\n\n\nWilkinson, R, M M Mleczko, R J W Brewin, K J Gaston, M Mueller, J D Shutler, X Yan, and K Anderson. 2024. “Environmental Impacts of Earth Observation Data in the Constellation and Cloud Computing Era.” Science of The Total Environment 909. https://doi.org/10.1016/j.scitotenv.2023.168584.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started with remote sensing</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 - Learning Diary (JTSE)",
    "section": "",
    "text": "Welcome!\n\n\n\n\n\n\n\n\n\nHi, my name is James and this is my learning diary for CASA0023: Remote Sensing Cities and Environments. I am a postgraduate student current undertaking a MSc in Urban Spatial Science at The Barlett Centre for Advanced Spatial Analysis (with an expected graduation date of August 2024). My other modules cover urban simulation, data science, GIS, Python, R and agent-based modelling. My interests include hiking, drone piloting/photography, cycling, cooking and board games.\n\n\nFeel free to connect with me on LinkedIn :) Also open to discussing employment opportunities related to the above skillsets.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "Chapter 3.html#summary",
    "href": "Chapter 3.html#summary",
    "title": "3  Remote Sensing Data",
    "section": "",
    "text": "3.1.1 Geometric Correction\nDistortions in images could arise from a variety of sources and the implications are that the image is not representative of what is on the ground. In images, shape remain accurate directly below the sensor (nadir) but distortion occurs towards the edges of the image (Toutin 2011). The distance from the sensor to the ground is further at the edges than at the nadir resulting in a greater compression of the features.\n\n\n\n\n\n\n\n\n\nShape preservation at the nadir and distortion towards the edges. Source: Government of Canada (2015)\nOther causes of distortion could include inconsistency in the altitude and velocity of the sensors (Earth and Mineral Science, n.d.). As the Earth rotates on its axis and satellites orbit around the Earth, this results in the image being skewed. This is due to each scan row covering an area slightly west of the preceding image.\nTo correct this issue, Ground Control Points (GCPs) need to be identified to match known points (e.g. prominent landmarks, buildings, intersections) in the image with a reference dataset which could be in the form of a local map or another image.\n\n\n\n\n\n\n\n\n\nCreating ground control points to match uncorrected image to reference image prior to applying polynomial transformation. Source: Basith (2011)\nA geometric transformation is then applied to each pixel for rectification purpose. There are two modelling methods: 1) input to output (forward) mapping and 2) output to input (inverse) mapping (Jensen 2015). In the former, pixels in the original image are transformed to have new coordinates in the reference image based on a formula created from the GCPs. However, a possible issue may arise when the transformed coordinates do not correspond with integer coordinates in the reference image and hence may have missing values. The latter of output to input (backward mapping) is able to overcome this issue. This process takes a value in the reference image, inputs it into the equation and returns a value in the original image, thus ensuring that there is a new value in the latter. As a check if the pixels between the original and reference image are aligned, the root mean squared error (RMSE) can provide an indication of the best fit. A lower RMSE is preferred with some studies suggesting a threshold of 0.5 or less (Jensen 2015). Lower RMSE can be achieved by adding more ground control points. Lastly, there may be some misalignment between the original grid and new grid in the transformation process and hence there is a need for resampling of interpolating new pixel values based on the original pixel values. Resampling techniques include nearest neighbour, linear, cubic and cubic spline.\n\n\n3.1.2 Contrast Enhancement\nDifferent surfaces (e.g. soil, vegetation, urban) reflect comparable amount of energy in the visible, near-infrared and middle-infrared portions of the electromagnetic spectrum (Jensen 2015). As such, it remains challenging to visually distinguish between various materials. The reason for this low contrast is due to images using only brightness values between 4 and 105, thereby neglecting brightness values in the ranges of 0 and 3, and 106 and 255. Various methods to expand the range and utilise these values to increase the contrast include 1) minimum-maximum, 2) percentage linear and standard deviation and 3) piecewise linear contrast stretch.\nMinimum-maximum method is usually applied to remote sensing images where the distribution of the pixel values assume a normal or near normal distribution. Any pixel with a brightness value of 4 would be assigned a bright value of 0 now while any pixel with a brightness value of 105 would be given a brightness value of 255. In other words, the brightness values are now linearly distributed between 0 and 255 instead of 5 and 105 thus increasing the contrast.\nPercentage linear and standard deviation is similar to the minimum-maximum method where a certain percentage or standard deviation of pixels from the mean are used instead. The brightness value within these specified minimum and maximum values are now linearly distributed between 0 and 255 instead. Brightness values below the lower and above the upper bound of the percentage or standard deviation, are mapped to the minimum and maximum of the dynamic range. As such, there are more black and white pixels thus creating a larger contrast (Al-amri, Kalyankar, and Khamitkar 2010).\n\n\n\n\n\n\n\n\n\nFrom top to bottom row: Original Landsat Image of Charleston, South Carolina and corresponding histogram; Minimum-maximum contrast stretch applied to image and corresponding histogram; Standard deviation contrast stretch and corresponding histogram. Source: Jensen (2015)\nPiecewise linear contrast is used when the pixels do not assume a normal distribution such as assuming a bimodal or trimodal pattern. In this case, breaks in the histogram of brightness values need to be identified and different transformations applied to stretch the values. Piecewise linear contrast is used to exaggerate or highlight specific features in the images such as a river while making the surrounding land appear black.\n\n\n\n\n\n\n\n\n\nPiecewise linear contrast stretch used to exaggerate Savannah River and corresponding histogram. Source: Jensen (2015)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "Chapter 5.html",
    "href": "Chapter 5.html",
    "title": "5  Google Earth Engine",
    "section": "",
    "text": "5.1 Summary\nGoogle Earth Engine (GEE) is a one-stop platform providing access to satellite images spanning over 30 years, including products from Sentinel, Landsat, MODIS, and other satellite missions (Google Earth Engine, n.d.b). GEE has a web-based code editor in JavaScript to perform different kinds of analysis or algorithms. As the code is run on the backend or on the Google servers, GEE becomes more accessible as users do not need to rely on their local devices to run potentially computationally intensive algorithms. GEE has experienced a meteoric rise in popularity since its launch in 2015, outpacing many other geospatial analysis software within 5 years, as evidenced by the number of journal articles utilising it.\nNumber of Journal Articles Using Different Geospatial Analysis Software. Source: Ballinger (2024)\nVarious processes that can be performed in GEE include:\nTo elaborate more on the linear regression process, it aims to model the relationship between dependent variable and independent variables. The example cited in class involved loading an image collection of NASA climate scenarios (NASA NEX-DCP30), filtering for the RCP 8.5 scenario (see CarbonBrief (2019) for more) and date range from 2006 to 2050 (Google Earth Engine, n.d.c). The linear regression was run with precipitation as the dependent variable and time as the independent variable. The map generated shows areas for continental US with the projected increase in precipitation in blue and the projected decreased in precipitation in red.\nMap showing projected precipitation for continental US after applying linearFit() to the projected precipitation. Source: (Google Earth Engine, n.d.c)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Chapter 5.html#summary",
    "href": "Chapter 5.html#summary",
    "title": "5  Google Earth Engine",
    "section": "",
    "text": "Spatial operations such as joins, zonal statistics and filtering\nReducing images by region(s) or neighbourhood\nMachine learning\nSupervised and unsupervised classification\nLinear regression/multivariate multiple linear regression\nAnd the list goes on…",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Chapter 5.html#application",
    "href": "Chapter 5.html#application",
    "title": "5  Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application\nThis week’s practical was my first foray into using GEE. It took some time to get used to the syntax in JavaScript and we mosaicked and clipped images, performed texture measures, principal component analysis (PCA) and band maths. Below are some of the JavaScript codes and outputs generated for the study area of Delhi.\nThe glcmTexture() function was used to “compute texture metrics from the Gray Level Co-occurrence Matrix around each pixel of each band” (Google Earth Engine, n.d.a). The brighter purple areas indicate high reflectance, typically found in built-up urban areas or industrial sites.\n\n\n\n\n\n\n\n\n\nOutput of applying glcmTexture() function to Delhi. Source: Author’s Own\nNormalised Difference Vegetation Index (NDVI) is used to understand vegetation density and health (USGS, n.d.). NDVI is calculated as a ratio between the red (R) and near infrared (NIR) bands. The highest areas or darker green areas corresponds to forests and parks.\n\n\n\n\n\n\n\n\n\nOutput of applying NDVI calculation to Delhi. Source: Author’s Own\nAs mentioned earlier, there has been increased interest in GEE. GEE can be used for vegetation monitoring, landcover mapping, agricultural applications and disaster management, etc. (Mutanga and Kumar 2019). GEE has the advantages of a large range of datasets, handling and processing huge data sets and applying various algorithms.\nA specific study involves estimating the gross primary productivity (GPP) over croplands (He et al. 2018). MODIS has a short revisit period but coarse spatial resolution. On the other hand, Landsat has a longer revisit cycle but finer spatial resolution. To provide accurate estimations, a fused NDVI dataset can be constructed by combining both Landsat 5 or 7 and Terra MODIS reflectance data. Google Earth Engine has the advantage of integrating different satellite products in its platform. Furthermore, as the study utilises a substantial amount of data from 2008 to 2015. GEE helps in the computation process of developing the fused NDVI map and calculating the GPP for the crop types in the study area.\nAnother study that shows the computation power of GEE is the timelapse of forest cover change in the Amazon Forest (TheVerge 2023). If the below classification was performed on a single computer for a 12-year period, the estimated processing time would be approximately 15 years. In comparison, the process would take only a few days at most in GEE.\n\n\n\n\n\n\n\n\n\nTimelapse of deforestation in Brazilian Amazon from 1985 to 2021. Source: TheVerge (2023)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Chapter 5.html#reflection",
    "href": "Chapter 5.html#reflection",
    "title": "5  Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nGEE is quite a powerful platform. It is easy to access different data products from Sentinel, Landsat, MODIS, etc. rather than manually downloading files from the respective websites. The platform takes awhile to get used as I have not used Javascript prior to this and seems like additional code is required to add features such as legend rather than clicking buttons in QGIS. Another thing that was puzzling was that one has to run the entire script instead of relevant code chunks each time so hopefully a patch comes out to improve the process. I am excited what other different processes I can execute in GEE in the weeks to come.\n\n\n\n\nBallinger, Ollie. 2024. “CASA0025: Building Spatial Applications with Big Data.” https://oballinger.github.io/CASA0025/.\n\n\nCarbonBrief. 2019. “Explainer: The High-Emissions ‘RCP8.5’ Global Warming Scenario.” https://www.carbonbrief.org/explainer-the-high-emissions-rcp8-5-global-warming-scenario/.\n\n\nGoogle Earth Engine. n.d.a. “Ee.image.glcmTexture.” https://developers.google.com/earth-engine/apidocs/ee-image-glcmtexture.\n\n\n———. n.d.b. “Google Earth Engine.” https://earthengine.google.com/.\n\n\n———. n.d.c. “Linear Regression.” https://developers.google.com/earth-engine/guides/reducers_regression.\n\n\nHe, Mingzhu, John S. Kimball, Marco P. Maneta, Bruce D. Maxwell, Alvaro Moreno, Santiago Beguería, and Xiaocui Wu. 2018. “Regional Crop Gross Primary Productivity and Yield Estimation Using Fused Landsat-MODIS Data.” Remote Sensing 10 (3): 372. https://doi.org/10.3390/rs10030372.\n\n\nMutanga, Onisimo, and Lalit Kumar. 2019. “Google Earth Engine Applications.” Remote Sensing 11 (5): 591. https://doi.org/10.3390/rs11050591.\n\n\nTheVerge. 2023. “How Google Earth Engine Revolutionized the Way We Monitor Deforestation.” https://www.theverge.com/23746844/google-earth-engine-amazon-deforestation-monitoring.\n\n\nUSGS. n.d. “Landsat Normalized Difference Vegetation Index.” https://www.usgs.gov/landsat-missions/landsat-normalized-difference-vegetation-index.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Chapter 6.html",
    "href": "Chapter 6.html",
    "title": "6  Classification I",
    "section": "",
    "text": "6.1 Summary\nCART refers to “Decision Tree algorithms that can be used for classification or regression predictive modeling problems” (Brownlee 2020). CART is particularly relevant for remote sensing in classifying pixels into same groups based on their similarities. While CART shares some similarities with clustering analysis in terms of their objectives, it differs in that CART focuses more on predicting future datasets rather than explaining the data (Bakker 2015). CART is also hierarchical, as it is top-down and splits pixels into their groups, while trying to minimize the Gini impurity for categorical data, which measures the likelihood of new data being misclassified if it was labeled randomly and independently (Karabiber, n.d.), or mean squared error (MSE) for continuous data, which measures the accuracy of the tree (Minitab, n.d.). This process continues until a specific stop condition has been met, such as max tree depth or minimal instances in a node, thus producing a decision-tree structure (Yadav 2019). On the other hand, clustering is agglomerative in moving from the bottom-up in sorting pixels into a set of clusters (Bakker 2015). An example of the decision tree can be seen below, starting from the root node and splitting into internal nodes until it reaches the lowest level which is the leaf node.\nCART Decision Tree Concept. Source: GeeksforGeeks (2023)\nAn example of the classification tree in remote sensing is shown below where B4 (NIR) is used initially for splitting before being split by B2, followed by other bands. This allows for classification of different land use land cover and the leaf nodes are discrete values of land use types such as agriculture, less vs. dense built up areas, etc.\nClassification Tree of Remote Sensing Image. Source: (Sharma, Ghosh, and JOSHI 2013)\nAn example of the regression tree is shown below where elevation, followed by understory cover, stand age, tree height, slope, aspect are used to calculate the percentage or burnt severity of forest areas. Compared to the classification tree, the leaf nodes would take the continuous values instead.\nRegression Tree of Remote Sensing Image. Source: Wu et al. (2013)\nThe advantages of CART include that the algorithm assists in the splitting and is easy to interpret from the flowchart. It can also be applied to data which may not be linear or data that is continuous or categorical. However, it has disadvantages, such as being prone to overfitting on the training data. This can be mitigated by pruning the tree or simplifying it by removing sections of the tree that are not critical to classification. For more advantages and disadvantages, read Inside Learning Machines (2023).\nOverall, as decision trees may not be best with new data, random forest algorithm of creating multiple decision tress instead of one, would be more useful.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification I</span>"
    ]
  },
  {
    "objectID": "Chapter 6.html#summary",
    "href": "Chapter 6.html#summary",
    "title": "6  Google Earth Engine",
    "section": "",
    "text": "Spatial operations such as joins, zonal statistics and filtering\nReducing images by region(s) or neighbourhood\nMachine learning\nSupervised and unsupervised classification\nLinear regression/multivariate multiple linear regression\nAnd the list goes on…",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Chapter 6.html#application",
    "href": "Chapter 6.html#application",
    "title": "6  Classification I",
    "section": "6.2 Application",
    "text": "6.2 Application\nThis week’s practical brings us to Shenzhen where we classified data using CART and Random Forest. We also utilised a train test split to train the model and test its accuracy on the test data. The results are as below.\n\n\n\n\n\n\n\n\n\nCART classification algorithm for Shenzhen (Green=Forest, Purple=Urban, Tan=Water, Red=Grass). Source: Author’s Own\nAs mentioned earlier, CART has an issue of overfitting. Random Forest attempts to address this issue by creating multiple decision trees based on random subsets of the training data (with replacement), with the final tree formed through voting or averaging (Belgiu and Drăguţ 2016). This variability improves the model’s classification performance and ability to generalise to new data.\n\n\n\n\n\n\n\n\n\nRandom Forest algorithm for Shenzhen (Green=Forest, Purple=Urban, Tan=Water, Red=Grass). Source: Author’s Own\nIt is a bit hard to see the difference in results but some of the grass areas are now classified correctly instead of forest. The validation overall accuracy is approximately 99.4% which represents the accuracy of the model on the testing dataset.\nRandom Forest Algorithm has been applied to Sentinel 2 Images to study fire severity in south-eastern Australia (Gibson et al. 2020). The study applied the algorithm for different indices such as differenced normalised burn ratio (dNBR), relativised change in fractional cover (RdFCT) and differenced normalised differenced vegetation index (dNDVI). The results were compared to aerial photograph interpretations (API) which revealed that the fractional cover indices were more accurate in differentiating burnt from non-burnt areas. However, accuracy varied across canopy types, with higher accuracy to when the entire crown was completely scorched as compared to when only the understory was burnt. This is expected as optical sensors face challenges in detecting burnt understory especially with higher canopy cover. This study shows that besides the inherent limitations of random forest (e.g. overfitting, computationally intensive), it is important to recognise the limitations of the dataset as well.\nRandom Forest has been used with remote sensing data for prediction purposes as demonstrated in a study that predicted droughts on a five-day period in East Asia (Park et al. 2018). The study used data from MODIS was used to calculate Land Surface Temperature (LST) and Normalised Difference Vegetation Index (NDVI), precipitation data from Goddard Earth Sciences Data and Information Centre and soil moisture from the European Space Agency, and real-time multivariate Madden-Julian oscillation (MJO) indices from the Australian Bureau of Meteorology. The maps exhibited spatial patterns similar to the actual drought index maps which is a step-in drought monitoring and warning systems. This could provide decision-makers with accurate and timely information to implement relevant measures.\n\n\n\n\n\n\n\n\n\nComparison of drought conditions between actual Vegetation Supply-Demand Index (VSDI) and predicted VSDIs from two models (with and without Madden-Julian Oscillation) in 2011. Source: Park et al. (2018)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification I</span>"
    ]
  },
  {
    "objectID": "Chapter 6.html#reflection",
    "href": "Chapter 6.html#reflection",
    "title": "6  Classification I",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nDuring my undergraduate, I did supervised classification in ENVI using maximum likelihood classification which I found to be a straightforward practical due to the intuitive interface. On hindsight, I better understand its limitations such as the requirement regarding knowing the probability of a pixel belonging to a specific land type. While CART and random forest are powerful tools for classification, the process can feel like a black box as we are unable to see what happens during the entire process from train-test split to model generation.\nAdditionally, many studies extend beyond classification alone, often combining classification with other methods or tools such as change detection for deforestation or prediction of natural hazards. With deeper understanding of how to utilise these tools and interpret their results, they can become useful tools to inform decision-makers.\n\n\n\n\nBakker, Jonathan D. 2015. “Overview of Classification and Regression Trees.” In. Switzerland: Springer. https://uw.pressbooks.pub/appliedmultivariatestatistics/chapter/classification-and-regression-trees/.\n\n\nBelgiu, Mariana, and Lucian Drăguţ. 2016. “Random Forest in Remote Sensing: A Review of Applications and Future Directions.” ISPRS Journal of Photogrammetry and Remote Sensing 114: 24–31. https://doi.org/10.1016/j.isprsjprs.2016.01.011.\n\n\nBrownlee, Jason. 2020. “Classification and Regression Trees for Machine Learning.” https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/.\n\n\nGeeksforGeeks. 2023. “CART (Classification and Regression Tree) in Machine Learning.” https://www.geeksforgeeks.org/cart-classification-and-regression-tree-in-machine-learning/.\n\n\nGibson, Rebecca, Tim Danaher, Warwick Hehir, and Luke Collins. 2020. “A Remote Sensing Approach to Mapping Fire Severity in South-Eastern Australia Using Sentinel 2 and Random Forest.” Remote Sensing of Environment 240: 111702. https://doi.org/10.1016/j.rse.2020.111702.\n\n\nInside Learning Machines. 2023. “8 Key Advantages and Disadvantages of Decision Trees.” https://insidelearningmachines.com/advantages_and_disadvantages_of_decision_trees/.\n\n\nKarabiber, Faith. n.d. “Gini Impurity.” https://www.learndatasci.com/glossary/gini-impurity/.\n\n\nMinitab. n.d. “Model Summary for CART® Regression.” https://support.minitab.com/en-us/minitab/20/help-and-how-to/statistical-modeling/predictive-analytics/how-to/cart-regression/interpret-the-results/all-statistics-and-graphs/model-summary-table/.\n\n\nPark, Seonyoung, Eunkyo Seo, Daehyun Kang, Jungho Im, and Myong-In Lee. 2018. “Prediction of Drought on Pentad Scale Using Remote Sensing Data and MJO Index Through Random Forest over East Asia.” Remote Sensing 10 (11): 1811. https://doi.org/10.3390/rs10111811.\n\n\nSharma, Richa, Aniruddha Ghosh, and P. K. JOSHI. 2013. “Decision Tree Approach for Classification of Remotely Sensed Satellite Data Using Open Source Support.” Journal of Earth System Science 122 (5): 1237–47. https://doi.org/10.1007/s12040-013-0339-2.\n\n\nWu, Zhiwei, Hong He, Yu Liang, Longyan Cai, and Bernard Lewis. 2013. “Determining Relative Contributions of Vegetation and Topography to Burn Severity from LANDSAT Imagery.” Environmental Management 52. https://doi.org/10.1007/s00267-013-0128-3.\n\n\nYadav, Ajay. 2019. “Decision Trees.” https://towardsdatascience.com/decision-trees-d07e0f420175.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification I</span>"
    ]
  },
  {
    "objectID": "Chapter 7.html",
    "href": "Chapter 7.html",
    "title": "7  Classification II",
    "section": "",
    "text": "7.1 Summary\nTo evaluate the accuracy of an output, we require certain metrics. This can be visualised through a confusion matrix which summarises the classification/misclassification of pixels for different classes and the image as a whole. User and Producer’s Accuracy are just two examples that can be seen in this matrix.\nSample confusion matrix. Green represents true values while orange represents false values. Source: Author’s own\nProducer Accuracy (read across rows for each class) refers to the percentage of pixels that are correctly classified (true positive) relative to the all pixels in that reference class (true positive + false negative). In other words, this is concerned with “errors of omission…or how accurately the classification results meet the expectation of the creator” (Esri, n.d.).\nUser Accuracy (read across columns for each class) refers to the percentage of pixels that are correctly classified (true positive) relative to all pixels classified as that particular land cover (true positive + false positive). In other words, this is concerned with “errors of commission, where pixels are incorrectly classified as a known class when they should have been classified as something different” (Esri, n.d.).\nThere is a trade-off in the attempting to balance between user and producer accuracy (Foody 2020). Increasing producer accuracy involves minimising false negatives by trying to correctly identify the pixels as belonging to a certain class. However, this may lead to the system generating too many false positives, which can lower user accuracy due to more incorrect predictions. On the other hand, to enhance user accuracy, the system becomes more conservative and only classifies pixels as belonging to a certain class if it highly confident. This approach aims to minimise false positive but may result in missing out pixels that actually belong to the class. For an interactive example demonstrating this trade off, refer to Wilber (2022).\nA comprehensive performance metric that considers both producer and user accuracy is the F1 score (Wilber 2022). It is calculated using the formula:\nF1 = \\(\\frac{2.Precision.Recall}{Precision+Recall}\\)\nwhere precision reflects user accuracy while recall reflects producer accuracy. Alternatively, the formula can be interpreted as:\nF1=\\(\\frac{True Positive}{True Positive+0.5.(False Positive+False Negative)}\\)\nDespite its utility, the F1 score has limitations as it fails to account for true negatives or accurately classified negative categories. The F1 score may hence be biased if there are more negative pixels than positive ones.\nAnother issue we covered was spatial autocorrelation. The underlying concept is that nearby pixels are more similar than distant pixels which may violate the assumption of independence and affect the model’s generalisability to other spatial contexts (Karasiak et al. 2022). To address this issue, ‘spatial partitioning’ is used to ensure the training and testing datasets are not geographically close to each other. In other words, the observations are split into spatially disjointed subsets using methods such as the k-means metric (Lovelace, Nowosad, and Muenchow 2019).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "Chapter 7.html#summary",
    "href": "Chapter 7.html#summary",
    "title": "7  Google Earth Engine",
    "section": "",
    "text": "Spatial operations such as joins, zonal statistics and filtering\nReducing images by region(s) or neighbourhood\nMachine learning\nSupervised and unsupervised classification\nLinear regression/multivariate multiple linear regression\nAnd the list goes on…",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Chapter 7.html#application",
    "href": "Chapter 7.html#application",
    "title": "7  Classification II",
    "section": "7.2 Application",
    "text": "7.2 Application\nIn the practical, we did subpixel, object-based, image and super pixels classification. Below is the subpixel classification output for Dar es Salaam, Tanzania, using Landsat 8 data. The general idea behind subpixel analysis is that many features are smaller than the size of the pixel, which may lead to inaccurate results or information loss (Suresh and Jain 2018). Subpixel analysis addresses this limitation by estimating and calculating the proportion of different land cover types within each pixel.\n\n\n\n\n\n\n\n\n\nSubpixel analysis for Dar es Salaam (Green=Forest, Pink=Urban, White=Grass). Source: Author’s Own\nOther applications in subpixel analysis include change detection of urban landcover in cities such as Shanghai and Xuzhou, particularly when dealing with satellite images of lower spatial resolutions and complex urban environments with multiple land uses (Du et al. 2014). In this approach, a spectral mixture model is applied to the imagery to decompose each pixel into its constituent endmember, in other words spectrally pure members such as water and vegetation. The process also estimates the proportion of each endmember within each pixel. Change detection is then carried out at a subpixel level to identify subtle changes in the land use within the pixels which may not be so apparent if one was to consider the pixel as a whole.\n\n\n\n\n\n\n\n\n\nLandcover change intensity in Shanghai from 2005 to 2009. Source: Du et al. (2014)\nSubpixel analysis can also be applied in the physical environment such as estimating river wetted width (RWW) to comprehend hydrological and biogeochemical processes. Challenges arise along river boundaries where pixels contain a mix of water and land covers, or the rivers have small width (Ling et al. 2019). To address these challenges, subpixel analysis, combined with superresoluton mapping, enables accurate mapping of the geographical extent of water bodies and reduces fractional errors in the classification process.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "Chapter 7.html#reflection",
    "href": "Chapter 7.html#reflection",
    "title": "7  Classification II",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nThis week’s lesson was a good reminder than features on the ground do not neatly fall within a pixel and hence there is a need for different classification methods such as subpixel and object-based. Furthermore, while the practical exercise focused on distinct classes such as urban, vegetation, water and bare earth, it is important to remember that the urban environment consists of many diverse land use such as informal settlements, factories, parks, schools, reservoirs. Although a repeat of last week’s reflection, classification is only part of the process of remote sensing and can be combined with other methods such as change detection.\nI also liked the throwback to account for spatial autocorrelation from the GIS module in my previous semester else there could be overfitting issues that would affect the model’s generalisability to new datasets.\n\n\n\n\nDu, Peijun, Sicong Liu, Pei Liu, Kun Tan, and Liang CHENG. 2014. “Sub-Pixel Change Detection for Urban Land-Cover Analysis via Multi-Temporal Remote Sensing Images.” Geo-Spatial Information Science 17 (1): 26–38. https://doi.org/10.1080/10095020.2014.889268.\n\n\nEsri. n.d. “Accuracy Assessment.” https://pro.arcgis.com/en/pro-app/3.1/help/analysis/image-analyst/accuracy-assessment.htm.\n\n\nFoody, Giles M. 2020. “Explaining the Unsuitability of the Kappa Coefficient in the Assessment and Comparison of the Accuracy of Thematic Maps Obtained by Image Classification.” Remote Sensing of Environment 239: 111630. https://doi.org/10.1016/j.rse.2019.111630.\n\n\nKarasiak, N., J.-F. Dejoux, C. Monteil, and D. Sheeren. 2022. “Spatial Dependence Between Training and Test Sets: Another Pitfall of Classification Accuracy Assessment in Remote Sensing.” Machine Learning 111 (7): 2715–40. https://doi.org/10.1007/s10994-021-05972-1.\n\n\nLing, Feng, Doreen Boyd, Yong Ge, Giles M. Foody, Xiaodong Li, Lihui Wang, Yihang Zhang, et al. 2019. “Measuring River Wetted Width from Remotely Sensed Imagery at the Subpixel Scale with a Deep Convolutional Neural Network.” Water Resources Research 55 (7): 5631–49. https://doi.org/10.1029/2018WR024136.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. “12 Statistical Learning.” In, 1st ed. London: Chapman & Hall. https://r.geocompx.org/spatial-cv.html.\n\n\nSuresh, M., and Kamal Jain. 2018. “Subpixel Level Mapping of Remotely Sensed Image Using Colorimetry.” The Egyptian Journal of Remote Sensing and Space Science 21 (1): 65–72. https://doi.org/10.1016/j.ejrs.2017.02.004.\n\n\nWilber, Jared. 2022. “Precision and Recall.” https://mlu-explain.github.io/precision-recall/.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "Chapter 8.html",
    "href": "Chapter 8.html",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "",
    "text": "8.1 Summary\nThere are two types of sensors: passive and active (GIS Geography 2015). Passive sensors measure electromagnetic radiation from the sun that is reflected by the earth. Passive sensors are present on Landsat and MODIS satellites. On the other hand, active sensors have their own energy source where they actively emit a pulse and measure the signal that is reflected from the earth. An example of an active sensor is Synthetic Aperture Radar (SAR).\nSAR obtains its name from combining a sequence of images from a shorter antenna to provide an image with higher spatial resolution (NASA 2020a). The spatial resolution of data is dependent on the ratio of the sensor wavelength to the length of the sensor’s antenna (ibid). However, it is unfeasible to have a long antenna in space, thus leading to the above solution to ‘synthesize’ a long antenna.\nThe radar is continuously in motion and covers an area on the ground based on the wavelength and antenna dimensions. A continuous stream of pulse is sent to the ground and reported back such that any point on the ground is sampled numerous times. Source: California Institute of Technology (n.d.)\nThe different wavelength for SAR determines its interaction with the earth’s surface and its capability to penetrate into a medium. An X-band with a shorter wavelength is unlikely to penetrate through thick vegetation and hence mainly interacts with the tree canopy whereas an L band is able to penetrate through and allow for backscattering from the branches and trunks below. A summary of the bands, wavelengths and applications of SAR are as follow:\nSummary of bands, wavelengths and applications of SAR. Source: NASA (2020a)\nBesides wavelengths, another consideration is polarisation which refers to “the orientation of the plane in which the transmitted electromagnetic wave oscillates” (NASA 2020a). SAR sensors can transmit signals in the horizontal (H) or vertical (V) polarisation and likewise receive them in either the horizontal (H) or vertical (V). As different surfaces are more sensitive or respond differently to the polarisations (VV, VH, HV, HH), analysing the signal strength in these polarisations can provide insight into the structure of the observed surface.\nVisualisation showing types of scattering for different features. Source: NASA (2020a)\nReferring to the image above\nThe interpretation of SAR images is not the most straightforward. The Alaska Satellite Facility conceptualised some rule of thumb on interpreting different features. For more information, read Alaska Satellite Facility (2019b).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperture Radar (SAR)</span>"
    ]
  },
  {
    "objectID": "Chapter 8.html#summary",
    "href": "Chapter 8.html#summary",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "",
    "text": "Strong scattering in the VV polarisation indicates a predominance of rough scattering. This suggests the presence of surfaces with coarse textures such as bare earth and rough water surfaces.\nStrong scattering in the VH or HV polarisation (i.e. cross polarisation) indicates sensitivity to volume scattering. This implies the dominance of volumetric features such as dense vegetation, in particular leaves and branches.\nStrong scattering in the HH polarisation, signifies a tendency towards double bounce scattering. This indicates the presence of features that cause a double bounce effect, such as buildings, tree trunks and calm water surfaces.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperture Radar (SAR)</span>"
    ]
  },
  {
    "objectID": "Chapter 8.html#application",
    "href": "Chapter 8.html#application",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "8.2 Application",
    "text": "8.2 Application\nThis week’s tutorial was one of my favourite which used SAR to perform a change detection of the Beirut port explosion in 2020 using Sentinel 1 image. SAR is useful for such tasks as it is able to detect subtle changes in amplitude and phase of radar wave reflection due to building damages, which are not easily observable in visible light imagery (NASA 2020b).\nThe image needs to be filtered for its ascending and descending orbit, as the satellite views the earth’s surface from different angles. The t-test can then be calculated for each orbit to examine if the change observed is statistically significant before the results are combined into a single image. The t-test involves comparing images in the year before the explosion to establish a baseline and images two months after the explosion to capture the explosion effects, while excluding reconstruction efforts. Validation using building footprint data was then performed to check if the extent of the damage was accurately predicted. The end result shows the number of damaged buildings was 9256, which was close to UN’s estimate of 10000 buildings. An explanation for this underestimation is that the satellite image provides a top-down point of view of the buildings. However, some buildings could still be standing with their roof intact, yet receive damage at the sides which are not captured by the satellite.\n\n\n\n\n\n\n\n\n\nDamage assessment of Beirut after the 2020 port explosion (Red=high damage, Yellow/Orange=medium damage, Green=low damage, Blue=no damage). Source: Author’s own\nAnother use of SAR is for oil spill detection (Chen et al. 2017). SAR is suitable for this task due to its wide coverage and all-weather conditions. Earlier studies focused on using single polarimetric SAR images, where oil slicks appear as dark areas due to the suppression of Bragg scattering from the ocean surface. However, distinguishing oil spills from other phenomena like biogenic slicks posed challenges. Various studies proposed other polarimetric features to classify oil spills such as standard deviation of copolarised phase difference, pedestrial height and copolarisation ratio. Chen et al. (2017) applied deep learning algorithms to optimise polarimetric feature sets for oil spill detection based on the 2011 Norwegian oil-on-water exercise. Their approach outperformed other methods such as support vector machine and artificial neural networks in distinguishing between different types of slicks. Overall, this study shows that SAR can be useful for obtaining more information about different surfaces and distinguish them based on their properties, along with the integration of machine learning to improve classification problems.\nInterferometric Synthetic Aperture Radar (InSAR) is an extension of SAR which combines multiple SAR images acquired from slightly different positions and times to create interferograms (USGS, n.d.). It can be used to study ground deformation such as at volcanic areas (Schaefer, Lu, and Oommen 2016). InSAR offers advantages over field monitoring, such as lower cost, lower risk and the ability to detect subtle ground deformation near volcanic areas that may not be easily observable. It also boasts high spatial resolution and continuous monitoring capabilities. By utilising InSAR for post-eruption of Pacaya volcano in Guatemala in 2010, subsidence and deformation can be detected. This information can contribute to understanding the stability of the volcano and aid in hazard assessment. UASVAR and ALOS-1 were used as complementary datasets to analyse activities before the eruptions, their evolution and the impact on the surrounding terrain. Beyond volcanic monitoring, in the field of earth observation and geoscience, InSAR can be used to study earthquakes and glaciers using similar methodologies. which follow the same methodology.\n\n\n\n\n\n\n\n\n\nDisplacement and deformation at Pacaya post-2010 eruption indicated by the negative values. Source: Schaefer, Lu, and Oommen (2016)\nThere are just so many applications of SAR. The Alaska Satellite Facility (2019a) has compiled the following map which shows the different uses of SAR around the world.\n&lt;/p&gt;",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperture Radar (SAR)</span>"
    ]
  },
  {
    "objectID": "Chapter 8.html#reflection",
    "href": "Chapter 8.html#reflection",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nSAR was quite an interesting topic to read about as compared to other satellites. There were more considerations such as polarisation, or whether the satellite was on an ascending or descending orbits. I was also fascinated by the various uses cases of SAR such as land subsidence, damage assessment, and its applications in archaeology. This made me wonder, given SAR’s valuable information and advantages, why is it avoided at times or considered disadvantageous?\nFrom the practical and through other readings, SAR images generally require more processing, for example, considering polarisation and the orbit direction. They cannot be visually interpreted easily like an image and may suffer from information loss due to shadow (Um 2021). Additionally, SAR cannot be used to calculate various spectral indices directly, such as NDVI, as it operates in the microwave portion of the electromagnetic spectrum and measures backscatter to the Earth. In other words, it cannot capture the spectral properties of various materials (Mazza et al. 2018).\nThis reflection marks the end of the course. I think I come a long way in learning about remote sensing back in 2018 in my penultimate year of undergraduate, where I was exposed to ENVI and ArcGIS and limited to the lab computers. It is amazing how I was able to run all the practical from the past weeks on my personal device and that data is more accessible. The range of techniques and topics covered in this course was broader, encompassing classification, change detection, machine learning and different satellites. Remote sensing is a really exciting field and I hope to apply it to more projects in the future.\nAlso special shout-outs to my profs for their amazing lectures and practicals. Their resources can be found at MacLachlan (2024) and Ballinger (2024).\n\n\n\n\nAlaska Satellite Facility. 2019a. “What Is SAR?” https://asf.alaska.edu/information/sar-information/what-is-sar/.\n\n\n———. 2019b. “How Do i Interpret SAR Images?” https://asf.alaska.edu/information/sar-information/how-do-i-read-sar-images/.\n\n\nBallinger, Ollie. 2024. “CASA0025: Building Spatial Applications with Big Data.” https://oballinger.github.io/CASA0025/.\n\n\nCalifornia Institute of Technology. n.d. “Overview | Get to Know SAR.” https://nisar.jpl.nasa.gov/mission/get-to-know-sar/overview.\n\n\nChen, Guandong, Yu Li, Guangmin Sun, and Yuanzhi Zhang. 2017. “Application of Deep Networks to Oil Spill Detection Using Polarimetric Synthetic Aperture Radar Images.” Applied Sciences 7 (10): 968. https://doi.org/10.3390/app7100968.\n\n\nGIS Geography. 2015. “Passive Vs Active Sensors in Remote Sensing.” https://gisgeography.com/passive-active-sensors-remote-sensing/.\n\n\nMacLachlan, Andy. 2024. “CASA0023 Remotely Sensing Cities and Environments.” https://andrewmaclachlan.github.io/CASA0023/.\n\n\nMazza, Antonio, Massimiliano Gargiulo, Giuseppe Scarpa, and Raffaele Gaetano. 2018. “Estimating the NDVI from SAR by Convolutional Neural Networks.” In, 1954–57. https://doi.org/10.1109/IGARSS.2018.8519459.\n\n\nNASA. 2020a. “What Is Synthetic Aperture Radar? | Earthdata.” https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\n———. 2020b. “Scientists Map Beirut Blast Damage.” https://earthobservatory.nasa.gov/images/147098/scientists-map-beirut-blast-damage.\n\n\nSchaefer, Lauren N., Zhong Lu, and Thomas Oommen. 2016. “Post-Eruption Deformation Processes Measured Using ALOS-1 and UAVSAR InSAR at Pacaya Volcano, Guatemala.” Remote Sensing 8 (1): 73. https://doi.org/10.3390/rs8010073.\n\n\nUm, Albert. 2021. “Introduction to Sentinel 1: SAR (Synthetic-Aperture Radar) Data.” https://albertum.medium.com/introduction-to-sentinel-1-sar-synthetic-aperture-radar-data-2cca22cb35a6.\n\n\nUSGS. n.d. “InSARsatellite-Based Technique Captures Overall Deformation \"Picture\".” https://www.usgs.gov/programs/VHP/insar-satellite-based-technique-captures-overall-deformation-picture.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperture Radar (SAR)</span>"
    ]
  }
]